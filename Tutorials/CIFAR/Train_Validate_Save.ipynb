{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validate and Save\n",
    "\n",
    "** Major bugs**\n",
    "\n",
    ">1) used **argmax** for **label_test** tensor while computing correct prediction. That was needed only for one-hot encoded labels of MNIST\n",
    "\n",
    ">2) Weights are not saved outside the session unless explicitly saved\n",
    "\n",
    "> 3) Encountered **inf in logits** error after 9300 iterations of training (**model 2**) \n",
    "\n",
    ">4) Sometimes the weights still approach zero and training breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tf(x):\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    with tf.Session(config=config) as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        out = sess.run(x)\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class FLAGS(object):\n",
    "    pass\n",
    "\n",
    "FLAGS.batch_size = 128\n",
    "FLAGS.data_dir = \"/home/sankaran/exercise/ML/TF-Exercise/Tutorials/CIFAR/cifar-10-batches-bin\"\n",
    "FLAGS.num_preprocess_threads = 16\n",
    "FLAGS.num_classes = 10\n",
    "FLAGS.dtype = tf.float32\n",
    "FLAGS.train = True\n",
    "\n",
    "def distorted_inputs(data_dir, batch_size, distort=True):\n",
    "    \n",
    "    if(FLAGS.train):\n",
    "        filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)]\n",
    "        print(\"using \", filenames)\n",
    "    else:\n",
    "        filenames = [os.path.join(data_dir, 'test_batch.bin')]\n",
    "        print(\"using \", filenames)\n",
    "        \n",
    "        \n",
    "    # Create a queue that produces the filenames to read.\n",
    "    filename_queue = tf.train.string_input_producer(filenames,seed=0)\n",
    "    \n",
    "    #Create FixedLenthRecord Reader with fixed bytes to read\n",
    "    record_bytes = 32*32*3+1 #32*32*3 image with 1 byte for label\n",
    "    reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
    "    \n",
    "    key, value = reader.read(filename_queue)\n",
    "    \n",
    "    ##Decode\n",
    "    decoded = tf.decode_raw(value, tf.uint8)\n",
    "    label = tf.strided_slice(decoded,[0],[1])\n",
    "    image = tf.strided_slice(decoded,[1],[record_bytes])\n",
    "    \n",
    "    \n",
    "    label = tf.cast(label,tf.int32)\n",
    "    label = tf.reshape(label,[1])\n",
    "    image = tf.reshape(image,[3,32,32])\n",
    "    image = tf.transpose(image,[1,2,0])\n",
    "    #image = tf.cast(image,tf.float32) ## DESTROYES IMAGE VIS\n",
    "    \n",
    "    ##PRE PROCESS\n",
    "    if(distort and FLAGS.train):\n",
    "        image = tf.random_crop(image, [24, 24, 3])\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_brightness(image,max_delta=0.4)\n",
    "        image = tf.image.random_contrast(image,lower=0.5,upper=1.8)\n",
    "    \n",
    "    image = tf.image.convert_image_dtype(image,dtype=FLAGS.dtype)\n",
    "    # Ensure that the random shuffling has good mixing properties.\n",
    "    min_fraction_of_examples_in_queue = 0.4\n",
    "    NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "    min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *\n",
    "                             min_fraction_of_examples_in_queue)\n",
    "    \n",
    "    images, label_batch = tf.train.shuffle_batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=FLAGS.num_preprocess_threads,\n",
    "        capacity=min_queue_examples + 3 * batch_size,\n",
    "        min_after_dequeue=min_queue_examples,\n",
    "        seed=0)\n",
    "    \n",
    "    return [images,label_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "    \n",
    "    def weight_variable(shape,std=0.1,dtype=FLAGS.dtype):\n",
    "      initializer = tf.truncated_normal_initializer(stddev=std, dtype=dtype)\n",
    "      return tf.get_variable(\"Weights\",shape,initializer=initializer,dtype=dtype)\n",
    "\n",
    "    def bias_variable(shape,const=0.0,dtype=FLAGS.dtype):\n",
    "      initializer = tf.constant_initializer(const,dtype)\n",
    "      return tf.get_variable(\"biases\",shape,initializer=initializer,dtype=dtype)\n",
    "    \n",
    "    print(\"input : \", images)\n",
    "    \n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        W_conv1 = weight_variable([5, 5, 3, 64],std=5e-2)\n",
    "        b_conv1 = bias_variable([64],const=0.0)\n",
    "\n",
    "        conv = tf.nn.conv2d(images,W_conv1,strides=[1,1,1,1],padding=\"SAME\")\n",
    "        h_conv1 = tf.nn.relu(conv + b_conv1)\n",
    "        \n",
    "        print(\"conv 1 : \",h_conv1)\n",
    "        \n",
    "    with tf.variable_scope(\"maxpool1_norm\"):\n",
    "        h_pool1 = tf.nn.max_pool(h_conv1,ksize=[1,3,3,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "        h_norm1 = tf.nn.lrn(h_pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "        \n",
    "        print(\"pool_norm 1 : \",h_norm1)\n",
    "        \n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        W_conv2 = weight_variable([5, 5, 64, 64],std=5e-2)\n",
    "        b_conv2 = bias_variable([64],const=0.1)\n",
    "\n",
    "        conv = tf.nn.conv2d(h_norm1,W_conv2,strides=[1,1,1,1],padding=\"SAME\")\n",
    "        h_conv2 = tf.nn.relu(conv + b_conv2)\n",
    "        \n",
    "        print(\"conv 2 : \",h_conv2)\n",
    "        \n",
    "    with tf.variable_scope(\"norm_maxpool2\"):\n",
    "        h_norm2 = tf.nn.lrn(h_conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "        h_pool2 = tf.nn.max_pool(h_norm2,ksize=[1,3,3,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "        \n",
    "        print(\"norm_pool 2 : \",h_pool2)\n",
    "        \n",
    "    with tf.variable_scope(\"Flatten\"):\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [FLAGS.batch_size, -1])\n",
    "        \n",
    "        print(\"flatten : \",h_pool2_flat)\n",
    "        \n",
    "    with tf.variable_scope(\"fc1\"):\n",
    "        W_fc1 = weight_variable([h_pool2_flat.shape[1].value, 384],std=0.04)\n",
    "        b_fc1 = bias_variable([384],const=0.1)\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        \n",
    "        print(\"fc1 : \",h_fc1)\n",
    "        \n",
    "    with tf.variable_scope(\"fc2\"):\n",
    "        W_fc2 = weight_variable([384, 192],std=0.04)\n",
    "        b_fc2 = bias_variable([192],const=0.1)\n",
    "        h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "        \n",
    "        print(\"fc2 : \",h_fc2)\n",
    "        \n",
    "    with tf.variable_scope(\"logit\"):\n",
    "        W_fc3 = weight_variable([192, FLAGS.num_classes],std=1/192.0)\n",
    "        b_fc3 = bias_variable([FLAGS.num_classes],const=0.0)\n",
    "        logit = tf.nn.relu(tf.matmul(h_fc2, W_fc3) + b_fc3)\n",
    "        \n",
    "        print(\"logit : \",logit)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizer(wd):\n",
    "    fc1_w = tf.trainable_variables(scope='fc1/Weights')[0]\n",
    "    wd_fc1 = tf.multiply(tf.nn.l2_loss(fc1_w), wd, name='fc1/weight_loss')\n",
    "\n",
    "    fc2_w = tf.trainable_variables(scope='fc2/Weights')[0]\n",
    "    wd_fc2 = tf.multiply(tf.nn.l2_loss(fc2_w), wd, name='fc2/weight_loss')\n",
    "    \n",
    "    return wd_fc1+wd_fc2\n",
    "\n",
    "def loss(logit,labels):\n",
    "    with tf.variable_scope(\"cross-entropy\"):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(labels,[FLAGS.batch_size]), logits=logit)\n",
    "        avg_cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "        return avg_cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learning_rate,decay_step,decay_rate,global_step):\n",
    "    \n",
    "    FLAGS.train = True\n",
    "    with tf.variable_scope(\"Input-queue-train\"):\n",
    "        images,labels = distorted_inputs(FLAGS.data_dir,FLAGS.batch_size,distort=False)\n",
    "        #tf.summary.image('images', images)\n",
    "\n",
    "    logit = inference(images)\n",
    "    tf.summary.histogram(\"Logits\",logit,collections=[\"Train\"])\n",
    "\n",
    "    ce_loss = loss(logit,labels)\n",
    "    tf.summary.scalar(\"mean_cross_entropy\",ce_loss,collections=[\"Train\"])\n",
    "\n",
    "    total_loss = ce_loss + regularizer(0.004)\n",
    "    tf.summary.scalar(\"total_loss\",total_loss,collections=[\"Train\"])\n",
    "\n",
    "    lr = tf.train.exponential_decay(\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    global_step=global_step,\n",
    "                                    decay_steps=decay_step,\n",
    "                                    decay_rate=decay_rate,\n",
    "                                    staircase=True)\n",
    "\n",
    "    tf.summary.scalar(\"learning_rate\",lr,collections=[\"Train\"])\n",
    "    opt = tf.train.GradientDescentOptimizer(lr)\n",
    "    grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "    train_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "    \n",
    "    return (train_op,ce_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy():\n",
    "    \n",
    "    FLAGS.train = False\n",
    "    with tf.variable_scope(\"Input-queue-test\"):\n",
    "        images_test,labels_test = distorted_inputs(FLAGS.data_dir,FLAGS.batch_size,distort=False)\n",
    "\n",
    "    logit = inference(images_test)\n",
    "    ce_loss = loss(logit,labels_test)\n",
    "    tf.summary.scalar(\"test_mean_cross_entropy\",ce_loss,collections=[\"Test\"])\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(logit, 1), tf.reshape(tf.cast(labels_test,dtype=tf.int64),[FLAGS.batch_size,]))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    ac_summary = tf.summary.scalar(\"accuracy\",accuracy,collections=[\"Test\"])\n",
    "    tf.add_to_collection(\"acc\",accuracy)\n",
    "    \n",
    "    return (accuracy,ce_loss,tf.argmax(logit, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using  ['/home/sankaran/exercise/ML/TF-Exercise/Tutorials/CIFAR/cifar-10-batches-bin/data_batch_1.bin', '/home/sankaran/exercise/ML/TF-Exercise/Tutorials/CIFAR/cifar-10-batches-bin/data_batch_2.bin', '/home/sankaran/exercise/ML/TF-Exercise/Tutorials/CIFAR/cifar-10-batches-bin/data_batch_3.bin', '/home/sankaran/exercise/ML/TF-Exercise/Tutorials/CIFAR/cifar-10-batches-bin/data_batch_4.bin', '/home/sankaran/exercise/ML/TF-Exercise/Tutorials/CIFAR/cifar-10-batches-bin/data_batch_5.bin']\n",
      "input :  Tensor(\"Input-queue-train/shuffle_batch:0\", shape=(128, 32, 32, 3), dtype=float32)\n",
      "conv 1 :  Tensor(\"conv1/Relu:0\", shape=(128, 32, 32, 64), dtype=float32)\n",
      "pool_norm 1 :  Tensor(\"maxpool1_norm/norm1:0\", shape=(128, 16, 16, 64), dtype=float32)\n",
      "conv 2 :  Tensor(\"conv2/Relu:0\", shape=(128, 16, 16, 64), dtype=float32)\n",
      "norm_pool 2 :  Tensor(\"norm_maxpool2/MaxPool:0\", shape=(128, 8, 8, 64), dtype=float32)\n",
      "flatten :  Tensor(\"Flatten/Reshape:0\", shape=(128, 4096), dtype=float32)\n",
      "fc1 :  Tensor(\"fc1/Relu:0\", shape=(128, 384), dtype=float32)\n",
      "fc2 :  Tensor(\"fc2/Relu:0\", shape=(128, 192), dtype=float32)\n",
      "logit :  Tensor(\"logit/Relu:0\", shape=(128, 10), dtype=float32)\n",
      "using  ['/home/sankaran/exercise/ML/TF-Exercise/Tutorials/CIFAR/cifar-10-batches-bin/test_batch.bin']\n",
      "input :  Tensor(\"Input-queue-test/shuffle_batch:0\", shape=(128, 32, 32, 3), dtype=float32)\n",
      "conv 1 :  Tensor(\"conv1_1/Relu:0\", shape=(128, 32, 32, 64), dtype=float32)\n",
      "pool_norm 1 :  Tensor(\"maxpool1_norm_1/norm1:0\", shape=(128, 16, 16, 64), dtype=float32)\n",
      "conv 2 :  Tensor(\"conv2_1/Relu:0\", shape=(128, 16, 16, 64), dtype=float32)\n",
      "norm_pool 2 :  Tensor(\"norm_maxpool2_1/MaxPool:0\", shape=(128, 8, 8, 64), dtype=float32)\n",
      "flatten :  Tensor(\"Flatten_1/Reshape:0\", shape=(128, 4096), dtype=float32)\n",
      "fc1 :  Tensor(\"fc1_1/Relu:0\", shape=(128, 384), dtype=float32)\n",
      "fc2 :  Tensor(\"fc2_1/Relu:0\", shape=(128, 192), dtype=float32)\n",
      "logit :  Tensor(\"logit_1/Relu:0\", shape=(128, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = tf.placeholder(FLAGS.dtype)\n",
    "decay_step = tf.placeholder(tf.int32)\n",
    "decay_rate = tf.placeholder(FLAGS.dtype)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "\n",
    "train_op,train_loss = train(learning_rate,decay_step,decay_rate,global_step)\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True): \n",
    "  test_op,test_loss,prediction = eval_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {learning_rate:0.1,\n",
    "            decay_rate : 0.1,\n",
    "            decay_step:10000}\n",
    "\n",
    "max_steps = 5000\n",
    "eval_every = 2000\n",
    "max_eval_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0 Loss :  2.30e+00\n",
      "True\n",
      "100 Loss :  2.27e+00\n",
      "True\n",
      "200 Loss :  2.30e+00\n",
      "True\n",
      "300 Loss :  2.17e+00\n",
      "True\n",
      "400 Loss :  2.09e+00\n",
      "True\n",
      "500 Loss :  2.03e+00\n",
      "True\n",
      "600 Loss :  2.01e+00\n",
      "True\n",
      "700 Loss :  1.78e+00\n",
      "True\n",
      "800 Loss :  1.89e+00\n",
      "True\n",
      "900 Loss :  1.65e+00\n",
      "True\n",
      "1000 Loss :  1.70e+00\n",
      "True\n",
      "1100 Loss :  1.65e+00\n",
      "True\n",
      "1200 Loss :  1.59e+00\n",
      "True\n",
      "1300 Loss :  1.58e+00\n",
      "True\n",
      "1400 Loss :  1.66e+00\n",
      "True\n",
      "1500 Loss :  1.50e+00\n",
      "True\n",
      "1600 Loss :  1.71e+00\n",
      "True\n",
      "1700 Loss :  1.62e+00\n",
      "True\n",
      "1800 Loss :  1.27e+00\n",
      "True\n",
      "1900 Loss :  1.56e+00\n",
      "True\n",
      "2000 Loss :  1.28e+00\n",
      "Evaluating..\n",
      "False\n",
      "Testing : Loss :  1.68e+00  Accuracy :  4.61e-01 Prediction :  [0 6 0 0 2 9 2 6 0 8 9 8 0 6 2 6 5 9 0 2 0 6 5 5 8 6 6 4 0 8 8 7 7 9 9 4 8\n",
      " 0 7 6 5 8 5 7 0 7 7 5 8 8 7 2 7 8 9 7 5 2 0 6 4 0 6 9 0 0 8 5 8 8 7 0 7 0\n",
      " 0 7 6 0 7 8 9 0 8 9 0 4 0 4 5 6 9 5 0 9 7 8 0 0 2 2 9 0 7 4 7 7 2 9 0 8 4\n",
      " 5 6 8 4 4 8 7 8 8 5 0 7 7 0 4 5 9]\n",
      "Testing : Loss :  1.37e+00  Accuracy :  5.55e-01 Prediction :  [0 2 8 6 0 8 7 5 6 8 5 4 6 6 4 7 7 4 6 5 8 7 5 4 6 8 7 9 4 8 6 8 8 0 8 5 9\n",
      " 4 6 6 7 6 2 0 9 0 7 6 6 6 0 8 7 0 6 5 0 5 0 9 0 6 4 8 0 5 8 8 9 0 0 2 2 0\n",
      " 6 5 8 8 0 7 4 5 7 4 4 8 9 7 6 9 4 6 6 9 5 8 8 6 8 6 4 2 6 8 4 9 0 9 9 6 6\n",
      " 0 6 8 5 0 8 9 8 4 6 8 5 4 7 9 8 2]\n",
      "Testing : Loss :  1.43e+00  Accuracy :  5.78e-01 Prediction :  [8 6 8 0 8 5 8 9 4 4 6 0 8 8 5 8 0 7 5 7 7 5 8 5 2 7 7 7 9 0 8 8 6 5 0 7 7\n",
      " 8 9 7 0 6 8 6 0 4 8 6 8 5 9 8 8 0 0 9 8 2 2 6 6 7 8 2 8 6 2 4 5 9 0 7 5 5\n",
      " 0 8 6 0 9 2 8 4 9 5 5 9 6 6 9 9 7 0 9 9 6 9 8 5 6 6 8 7 4 6 8 8 2 8 0 5 2\n",
      " 5 6 5 2 9 7 8 4 9 6 5 2 9 9 8 6 9]\n",
      "Testing : Loss :  1.35e+00  Accuracy :  5.62e-01 Prediction :  [4 0 7 9 8 7 5 9 2 0 8 8 6 4 5 6 9 0 8 6 5 9 0 9 6 8 6 7 8 2 2 6 5 6 9 4 0\n",
      " 8 9 2 7 9 4 4 8 0 4 6 5 9 8 0 2 5 6 8 7 6 4 6 6 5 5 4 9 9 5 7 8 8 8 8 9 6\n",
      " 0 4 0 4 7 8 6 8 8 6 0 8 7 0 4 2 8 6 8 6 5 6 5 9 6 9 0 8 0 7 5 7 7 0 0 8 9\n",
      " 6 8 7 0 9 0 2 2 8 9 0 9 0 8 9 5 4]\n",
      "Testing : Loss :  1.41e+00  Accuracy :  5.78e-01 Prediction :  [6 9 0 9 7 0 7 4 9 7 6 7 0 0 0 0 6 7 5 8 0 4 8 0 7 6 0 6 0 6 4 8 7 7 4 7 8\n",
      " 0 5 5 0 6 4 0 0 4 8 4 0 2 0 4 7 8 9 7 7 6 8 6 8 0 0 7 5 9 4 8 0 8 7 9 2 0\n",
      " 7 0 0 0 6 0 5 7 7 6 7 2 9 0 8 4 6 9 4 7 2 2 7 8 0 9 6 0 6 6 7 5 6 8 0 6 2\n",
      " 6 4 7 5 9 8 0 4 8 9 2 0 0 7 6 4 0]\n",
      "Testing : Loss :  1.52e+00  Accuracy :  5.00e-01 Prediction :  [9 0 0 6 5 6 8 9 0 0 4 2 6 0 6 0 4 7 4 0 8 0 6 0 7 0 7 9 0 4 5 4 7 9 7 0 8\n",
      " 9 7 6 5 9 4 8 8 5 7 8 9 0 6 7 7 4 9 0 5 4 0 8 9 8 8 5 6 5 6 9 9 6 7 9 9 0\n",
      " 7 2 0 0 8 2 4 7 0 5 2 4 4 6 0 0 0 9 4 6 6 9 4 8 8 7 7 6 9 8 9 8 2 2 8 5 8\n",
      " 7 4 6 8 6 5 6 8 0 8 7 2 8 6 8 6 9]\n",
      "Testing : Loss :  1.33e+00  Accuracy :  5.47e-01 Prediction :  [0 7 5 7 8 6 7 6 0 4 7 8 2 0 5 7 8 8 6 8 6 7 2 5 0 7 8 2 8 8 7 8 8 4 8 5 0\n",
      " 6 9 2 7 8 6 0 9 6 4 6 5 8 0 9 2 4 7 8 7 0 8 0 4 2 0 7 0 6 0 9 8 7 5 4 9 6\n",
      " 7 2 8 7 9 7 8 6 2 0 0 0 9 5 4 4 7 6 6 0 8 7 0 2 5 7 6 7 0 0 6 0 7 8 4 7 6\n",
      " 7 9 6 4 4 4 5 4 6 2 9 4 0 4 8 6 0]\n",
      "Testing : Loss :  1.38e+00  Accuracy :  5.55e-01 Prediction :  [8 5 4 5 9 9 9 9 0 9 4 4 0 8 4 6 0 8 6 2 9 7 4 4 9 6 4 7 0 8 0 0 4 4 9 9 9\n",
      " 4 6 9 7 6 0 6 8 8 8 8 4 6 4 0 7 6 8 7 4 5 6 6 9 8 6 2 2 7 9 4 8 5 2 2 4 5\n",
      " 0 9 2 4 8 9 8 2 8 9 0 5 8 4 0 6 8 7 2 4 4 0 5 5 2 7 0 5 8 6 8 2 0 0 0 0 0\n",
      " 7 6 0 6 0 8 5 0 2 8 6 0 0 6 9 6 7]\n",
      "Testing : Loss :  1.28e+00  Accuracy :  5.86e-01 Prediction :  [7 0 7 5 2 6 9 0 8 7 7 5 6 4 0 4 8 6 7 4 7 7 4 8 5 7 7 8 6 7 9 7 0 0 7 5 0\n",
      " 8 8 4 5 2 0 5 4 8 0 6 6 4 4 5 9 8 7 7 6 5 6 6 8 9 8 7 0 6 6 5 0 6 4 2 7 8\n",
      " 6 6 0 4 5 0 2 7 6 9 6 4 9 7 9 9 7 5 7 6 0 4 7 7 5 6 5 6 9 7 9 9 0 8 0 6 0\n",
      " 9 9 5 8 5 9 7 9 6 2 6 4 2 0 8 5 9]\n",
      "Testing : Loss :  1.47e+00  Accuracy :  5.23e-01 Prediction :  [0 6 8 7 4 2 8 8 4 7 0 9 7 8 8 6 2 6 0 6 7 5 8 7 0 4 7 9 6 0 2 7 2 4 4 0 6\n",
      " 9 6 7 5 0 9 4 6 7 8 4 2 2 4 2 6 2 2 0 6 0 7 7 2 5 8 5 0 0 0 6 9 0 7 0 4 2\n",
      " 6 6 5 8 0 8 9 2 8 6 8 8 6 0 6 5 0 7 7 4 0 8 7 6 5 7 8 0 0 0 0 2 6 2 0 0 5\n",
      " 6 4 0 0 4 8 2 7 6 2 8 6 7 4 5 8 2]\n",
      "True\n",
      "2100 Loss :  1.34e+00\n",
      "True\n",
      "2200 Loss :  1.51e+00\n",
      "True\n",
      "2300 Loss :  1.40e+00\n",
      "True\n",
      "2400 Loss :  1.31e+00\n",
      "True\n",
      "2500 Loss :  1.27e+00\n",
      "True\n",
      "2600 Loss :  1.28e+00\n",
      "True\n",
      "2700 Loss :  1.23e+00\n",
      "True\n",
      "2800 Loss :  1.26e+00\n",
      "True\n",
      "2900 Loss :  1.20e+00\n",
      "True\n",
      "3000 Loss :  1.10e+00\n",
      "True\n",
      "3100 Loss :  1.20e+00\n",
      "True\n",
      "3200 Loss :  1.10e+00\n",
      "True\n",
      "3300 Loss :  1.02e+00\n",
      "True\n",
      "3400 Loss :  1.32e+00\n",
      "True\n",
      "3500 Loss :  1.04e+00\n",
      "True\n",
      "3600 Loss :  1.18e+00\n",
      "True\n",
      "3700 Loss :  9.32e-01\n",
      "True\n",
      "3800 Loss :  9.74e-01\n",
      "True\n",
      "3900 Loss :  1.04e+00\n",
      "True\n",
      "4000 Loss :  1.10e+00\n",
      "Evaluating..\n",
      "False\n",
      "Testing : Loss :  1.38e+00  Accuracy :  5.39e-01 Prediction :  [2 7 5 0 5 6 4 7 0 5 9 9 9 6 4 4 6 5 6 6 0 4 7 0 9 8 9 0 9 8 9 9 9 0 5 5 0\n",
      " 7 9 5 7 0 0 0 4 0 0 4 7 4 5 6 7 5 6 5 0 7 4 7 0 6 6 8 7 9 7 9 0 0 4 5 7 4\n",
      " 0 0 9 7 4 7 5 0 5 6 6 9 0 9 4 5 7 9 4 7 5 0 5 7 4 7 6 2 4 9 9 4 8 9 9 0 0\n",
      " 7 4 6 7 0 7 8 5 6 4 9 0 9 2 6 4 9]\n",
      "Testing : Loss :  1.18e+00  Accuracy :  6.17e-01 Prediction :  [5 6 5 0 4 4 4 0 6 9 9 9 0 9 0 0 7 0 0 8 6 4 8 6 4 4 6 2 9 0 2 0 0 0 4 7 9\n",
      " 9 6 8 7 9 7 9 6 4 7 8 0 6 9 7 9 7 4 5 0 7 9 7 0 7 9 9 0 4 8 5 6 6 9 8 7 9\n",
      " 0 6 7 4 0 4 4 0 7 9 9 4 6 6 9 0 5 0 9 5 2 8 0 0 8 7 0 0 8 6 4 7 6 6 9 9 7\n",
      " 7 0 9 9 5 9 6 9 5 4 9 9 0 0 7 7 4]\n",
      "Testing : Loss :  1.42e+00  Accuracy :  5.39e-01 Prediction :  [8 7 7 9 4 4 0 0 0 9 9 2 7 9 6 4 6 7 6 0 0 4 7 0 6 5 4 7 6 9 9 0 7 9 4 7 2\n",
      " 7 8 0 7 8 7 6 7 7 8 0 2 0 0 9 6 9 4 0 5 6 7 9 7 6 4 8 6 9 7 9 5 7 6 6 4 0\n",
      " 5 7 9 7 5 0 4 5 0 7 4 8 4 0 6 7 7 0 5 9 9 7 7 5 0 5 2 8 5 4 9 9 4 8 9 9 7\n",
      " 6 7 6 4 4 7 2 7 5 9 5 4 0 7 7 8 7]\n",
      "Testing : Loss :  1.20e+00  Accuracy :  6.25e-01 Prediction :  [8 6 9 6 6 9 7 0 2 0 8 2 5 5 4 7 4 7 7 0 7 4 4 5 6 7 8 6 7 6 7 9 7 0 4 8 4\n",
      " 7 0 6 6 9 4 7 9 2 6 6 4 6 7 7 0 9 4 9 7 9 2 4 0 7 7 6 2 9 7 7 0 7 0 8 9 9\n",
      " 8 9 9 7 6 6 7 0 9 9 5 9 4 9 4 5 9 7 5 7 4 9 8 0 4 7 4 0 8 5 4 9 9 9 8 9 5\n",
      " 9 6 4 5 5 4 5 6 4 7 7 9 9 7 0 9 4]\n",
      "Testing : Loss :  1.33e+00  Accuracy :  6.02e-01 Prediction :  [9 7 4 7 9 8 4 8 4 5 8 6 5 7 4 6 7 8 2 8 9 6 7 7 9 5 0 0 4 8 9 5 6 6 0 4 9\n",
      " 7 8 9 6 7 8 4 4 6 0 9 4 8 5 9 7 8 6 4 4 7 7 4 0 0 5 7 6 4 9 7 9 0 0 7 8 7\n",
      " 5 6 9 9 0 9 5 4 7 9 7 8 9 8 4 0 5 7 0 9 6 2 4 9 9 0 8 6 0 7 0 6 0 4 7 7 0\n",
      " 7 0 7 6 6 9 6 0 9 0 6 4 8 0 9 0 0]\n",
      "Testing : Loss :  1.27e+00  Accuracy :  5.94e-01 Prediction :  [5 0 0 5 7 4 8 9 7 0 7 0 6 6 4 6 4 4 9 4 7 4 6 7 4 7 7 4 4 8 8 6 7 7 0 7 7\n",
      " 0 7 4 6 7 9 7 0 7 9 2 4 7 7 0 9 9 9 0 5 8 7 0 9 7 2 7 4 9 7 9 6 0 7 8 0 9\n",
      " 8 8 8 0 4 7 6 9 4 7 0 9 0 8 2 9 0 7 6 4 0 0 8 9 6 6 6 9 7 6 6 0 5 7 6 6 9\n",
      " 9 4 2 5 9 7 5 8 7 2 6 6 4 0 0 0 0]\n",
      "Testing : Loss :  1.45e+00  Accuracy :  5.47e-01 Prediction :  [7 5 2 4 9 0 8 5 0 4 8 5 0 7 9 6 7 7 0 7 0 7 7 7 4 7 4 8 5 8 4 5 7 4 8 4 9\n",
      " 7 7 6 7 6 0 9 7 9 8 2 0 5 6 7 0 4 5 0 8 5 2 7 7 9 5 9 9 4 6 9 9 7 7 9 9 9\n",
      " 8 4 5 0 7 0 6 7 9 8 4 4 9 9 7 7 7 7 9 0 7 5 8 6 0 7 7 7 0 4 5 4 0 7 4 7 0\n",
      " 6 6 0 7 0 9 0 7 0 4 9 8 4 9 7 7 9]\n",
      "Testing : Loss :  1.24e+00  Accuracy :  5.70e-01 Prediction :  [8 8 6 6 9 9 5 6 9 5 9 5 8 7 7 7 5 9 5 5 4 7 7 6 0 9 0 7 9 9 0 8 9 6 6 5 5\n",
      " 0 7 5 7 2 4 6 6 6 6 8 9 9 0 8 0 9 9 4 8 8 0 9 4 7 9 7 0 6 0 4 9 9 8 6 6 0\n",
      " 7 5 6 4 0 0 8 2 8 0 4 0 8 8 7 8 0 0 9 8 0 5 0 0 4 9 5 7 7 4 2 6 9 0 0 8 9\n",
      " 7 7 6 9 5 9 6 4 0 2 9 7 5 9 5 8 4]\n",
      "Testing : Loss :  1.35e+00  Accuracy :  5.94e-01 Prediction :  [0 6 6 8 4 5 9 0 5 9 7 5 8 8 5 9 9 9 7 7 7 5 0 4 9 9 9 0 7 0 0 5 9 6 7 0 0\n",
      " 2 7 7 0 9 5 6 9 6 6 5 6 6 5 9 0 0 2 0 4 0 6 5 7 7 7 6 9 4 5 9 5 6 9 0 5 8\n",
      " 9 5 7 5 5 6 6 6 4 0 6 2 6 4 9 5 4 9 4 9 2 5 5 8 9 4 0 4 4 6 5 5 7 5 9 5 7\n",
      " 9 4 8 8 5 9 7 8 5 0 8 4 9 6 5 9 9]\n",
      "Testing : Loss :  1.38e+00  Accuracy :  5.47e-01 Prediction :  [4 0 0 4 4 8 2 7 4 6 4 7 0 4 9 7 2 7 5 2 6 0 7 9 7 0 5 4 6 8 4 9 8 8 0 6 9\n",
      " 5 2 4 5 0 8 9 0 0 4 0 9 6 0 0 0 4 8 4 7 7 7 4 7 0 9 0 6 7 5 6 4 4 7 8 2 7\n",
      " 4 4 8 7 7 8 8 4 7 7 7 7 0 9 9 0 7 7 0 6 5 4 9 5 8 0 4 8 8 7 6 9 6 7 8 5 5\n",
      " 0 2 5 0 4 7 8 8 0 4 9 0 0 2 8 6 0]\n",
      "True\n",
      "4100 Loss :  1.08e+00\n",
      "True\n",
      "4200 Loss :  9.36e-01\n",
      "True\n",
      "4300 Loss :  9.01e-01\n",
      "True\n",
      "4400 Loss :  1.07e+00\n",
      "True\n",
      "4500 Loss :  1.03e+00\n",
      "True\n",
      "4600 Loss :  9.09e-01\n",
      "True\n",
      "4700 Loss :  8.55e-01\n",
      "True\n",
      "4800 Loss :  7.14e-01\n",
      "True\n",
      "4900 Loss :  8.74e-01\n"
     ]
    }
   ],
   "source": [
    "merged_train = tf.summary.merge_all(key=\"Train\")\n",
    "merged_test = tf.summary.merge_all(key=\"Test\")\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(config=config) as sess: \n",
    "    writer = tf.summary.FileWriter(\"Train_val_log/3/\",sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        FLAGS.train = True\n",
    "        _,tr_loss,summary = sess.run([train_op,train_loss,merged_train],feed_dict=feed_dict)\n",
    "        writer.add_summary(summary,i)\n",
    "        if(i%100 == 0):\n",
    "            print(FLAGS.train)\n",
    "            print(i, \"Loss : \", \"{:.2e}\".format(tr_loss))\n",
    "        \n",
    "        if(i%eval_every == 0 and i!=0):\n",
    "            FLAGS.train = False\n",
    "            print(\"Evaluating..\")\n",
    "            print(FLAGS.train)\n",
    "            for j in range(max_eval_steps):\n",
    "                acc,te_loss,pred,summary_te = sess.run([test_op,test_loss,prediction,merged_test],feed_dict=feed_dict)\n",
    "                writer.add_summary(summary_te,j)\n",
    "                if(j%100 == 0):\n",
    "                    print(\"Testing : Loss : \", \"{:.2e}\".format(te_loss), \" Accuracy : \",\"{:.2e}\".format(acc), \"Prediction : \", pred)\n",
    "                \n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,\"Train_val_log/3/models/model.chpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_name:  conv1/Weights\n",
      "tensor_name:  conv1/biases\n",
      "tensor_name:  conv2/Weights\n",
      "tensor_name:  conv2/biases\n",
      "tensor_name:  fc1/Weights\n",
      "tensor_name:  fc1/biases\n",
      "tensor_name:  fc2/Weights\n",
      "tensor_name:  fc2/biases\n",
      "tensor_name:  global_step\n",
      "tensor_name:  logit/Weights\n",
      "tensor_name:  logit/biases\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "chkp.print_tensors_in_checkpoint_file(\"Train_val_log/3/models/model.chpt\",tensor_name='', all_tensors=False,all_tensor_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Train_val_log/3/models/model.chpt\n"
     ]
    }
   ],
   "source": [
    "imported_meta = tf.train.import_meta_graph(\"Train_val_log/3/models/model.chpt.meta\")\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "imported_meta.restore(sess,tf.train.latest_checkpoint(\"Train_val_log/3/models/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['queue_runners',\n",
       " 'Test',\n",
       " 'train_op',\n",
       " 'summaries',\n",
       " 'trainable_variables',\n",
       " 'variables',\n",
       " 'global_step',\n",
       " 'acc',\n",
       " 'Train']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_all_collection_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sess.graph.get_collection(\"acc\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using  ['/home/sankaran/exercise/ML/TF-Exercise/Tutorials/CIFAR/cifar-10-batches-bin/test_batch.bin']\n",
      "input :  Tensor(\"shuffle_batch_1:0\", shape=(128, 32, 32, 3), dtype=float32)\n",
      "conv 1 :  Tensor(\"conv1_3/Relu:0\", shape=(128, 32, 32, 64), dtype=float32)\n",
      "pool_norm 1 :  Tensor(\"maxpool1_norm_3/norm1:0\", shape=(128, 16, 16, 64), dtype=float32)\n",
      "conv 2 :  Tensor(\"conv2_3/Relu:0\", shape=(128, 16, 16, 64), dtype=float32)\n",
      "norm_pool 2 :  Tensor(\"norm_maxpool2_3/MaxPool:0\", shape=(128, 8, 8, 64), dtype=float32)\n",
      "flatten :  Tensor(\"Flatten_3/Reshape:0\", shape=(128, 4096), dtype=float32)\n",
      "fc1 :  Tensor(\"fc1_3/Relu:0\", shape=(128, 384), dtype=float32)\n",
      "fc2 :  Tensor(\"fc2_3/Relu:0\", shape=(128, 192), dtype=float32)\n",
      "logit :  Tensor(\"logit_3/Relu:0\", shape=(128, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "FLAGS.train = False\n",
    "images_test,labels_test = distorted_inputs(FLAGS.data_dir,FLAGS.batch_size,distort=False)\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True): \n",
    "    logit = inference(images_test)\n",
    "correct_prediction = tf.equal(tf.argmax(logit, 1), tf.reshape(tf.cast(labels_test,dtype=tf.int64),[FLAGS.batch_size,]))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ True, False, False, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False, False, False,  True,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False, False, False, False,\n",
       "        False,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False, False, False, False,\n",
       "        False, False]),\n",
       " array([3, 4, 7, 7, 4, 7, 1, 8, 5, 3, 9, 9, 7, 4, 5, 8, 6, 3, 5, 1, 0, 2,\n",
       "        7, 7, 4, 0, 7, 5, 2, 7, 3, 6, 3, 4, 4, 9, 1, 7, 6, 6, 9, 2, 5, 5,\n",
       "        8, 8, 8, 9, 6, 5, 7, 1, 5, 6, 3, 4, 0, 1, 5, 2, 1, 5, 7, 4, 3, 2,\n",
       "        8, 2, 5, 5, 6, 0, 9, 7, 7, 5, 8, 8, 1, 0, 7, 4, 8, 9, 4, 2, 5, 7,\n",
       "        6, 3, 0, 9, 9, 0, 4, 7, 7, 4, 5, 8, 6, 0, 4, 4, 7, 5, 8, 5, 9, 4,\n",
       "        1, 0, 7, 0, 4, 0, 6, 3, 8, 0, 5, 4, 2, 0, 2, 8, 2, 6]),\n",
       " 0.0703125]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tf([correct_prediction,tf.reshape(tf.cast(labels_test,dtype=tf.int64),[FLAGS.batch_size,]),accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'logit_3/Relu:0' shape=(128, 10) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[5],\n",
       "        [6],\n",
       "        [3],\n",
       "        [7],\n",
       "        [4],\n",
       "        [6],\n",
       "        [6],\n",
       "        [9],\n",
       "        [5],\n",
       "        [2],\n",
       "        [9],\n",
       "        [5],\n",
       "        [1],\n",
       "        [0],\n",
       "        [6],\n",
       "        [2],\n",
       "        [4],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [5],\n",
       "        [5],\n",
       "        [6],\n",
       "        [4],\n",
       "        [1],\n",
       "        [9],\n",
       "        [6],\n",
       "        [2],\n",
       "        [0],\n",
       "        [2],\n",
       "        [1],\n",
       "        [4],\n",
       "        [9],\n",
       "        [7],\n",
       "        [4],\n",
       "        [0],\n",
       "        [5],\n",
       "        [7],\n",
       "        [9],\n",
       "        [4],\n",
       "        [0],\n",
       "        [4],\n",
       "        [6],\n",
       "        [1],\n",
       "        [2],\n",
       "        [8],\n",
       "        [0],\n",
       "        [4],\n",
       "        [1],\n",
       "        [3],\n",
       "        [0],\n",
       "        [0],\n",
       "        [2],\n",
       "        [4],\n",
       "        [1],\n",
       "        [7],\n",
       "        [3],\n",
       "        [0],\n",
       "        [7],\n",
       "        [6],\n",
       "        [6],\n",
       "        [4],\n",
       "        [1],\n",
       "        [3],\n",
       "        [9],\n",
       "        [0],\n",
       "        [8],\n",
       "        [2],\n",
       "        [5],\n",
       "        [2],\n",
       "        [9],\n",
       "        [5],\n",
       "        [0],\n",
       "        [3],\n",
       "        [0],\n",
       "        [4],\n",
       "        [2],\n",
       "        [0],\n",
       "        [7],\n",
       "        [3],\n",
       "        [4],\n",
       "        [6],\n",
       "        [8],\n",
       "        [0],\n",
       "        [2],\n",
       "        [4],\n",
       "        [7],\n",
       "        [4],\n",
       "        [2],\n",
       "        [4],\n",
       "        [9],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [6],\n",
       "        [0],\n",
       "        [6],\n",
       "        [0],\n",
       "        [9],\n",
       "        [4],\n",
       "        [2],\n",
       "        [5],\n",
       "        [5],\n",
       "        [3],\n",
       "        [8],\n",
       "        [1],\n",
       "        [4],\n",
       "        [2],\n",
       "        [4],\n",
       "        [3],\n",
       "        [3],\n",
       "        [4],\n",
       "        [2],\n",
       "        [9],\n",
       "        [3],\n",
       "        [4],\n",
       "        [8],\n",
       "        [3],\n",
       "        [2],\n",
       "        [9],\n",
       "        [2],\n",
       "        [3],\n",
       "        [6],\n",
       "        [2],\n",
       "        [5],\n",
       "        [1],\n",
       "        [6]], dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tf([labels_test,tf.argmax(labels_test,1)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
