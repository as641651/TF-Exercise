{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tf(x):\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    with tf.Session(config=config) as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        out = sess.run(x)\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class FLAGS(object):\n",
    "    pass\n",
    "\n",
    "FLAGS.batch_size = 128\n",
    "FLAGS.data_dir = \"/home/sankaran/exercise/ML/TF-Exercise/Tutorials/CIFAR/cifar-10-batches-bin\"\n",
    "FLAGS.num_preprocess_threads = 16\n",
    "FLAGS.num_classes = 10\n",
    "FLAGS.dtype = tf.float32\n",
    "\n",
    "def distorted_inputs(data_dir, batch_size, distort=True):\n",
    "    \n",
    "    filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)]\n",
    "    \n",
    "    # Create a queue that produces the filenames to read.\n",
    "    filename_queue = tf.train.string_input_producer(filenames,seed=0)\n",
    "    \n",
    "    #Create FixedLenthRecord Reader with fixed bytes to read\n",
    "    record_bytes = 32*32*3+1 #32*32*3 image with 1 byte for label\n",
    "    reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
    "    \n",
    "    key, value = reader.read(filename_queue)\n",
    "    \n",
    "    ##Decode\n",
    "    decoded = tf.decode_raw(value, tf.uint8)\n",
    "    label = tf.strided_slice(decoded,[0],[1])\n",
    "    image = tf.strided_slice(decoded,[1],[record_bytes])\n",
    "    \n",
    "    \n",
    "    label = tf.cast(label,tf.int32)\n",
    "    label = tf.reshape(label,[1])\n",
    "    image = tf.reshape(image,[3,32,32])\n",
    "    image = tf.transpose(image,[1,2,0])\n",
    "    #image = tf.cast(image,tf.float32) ## DESTROYES IMAGE VIS\n",
    "    \n",
    "    ##PRE PROCESS\n",
    "    if(distort):\n",
    "        image = tf.random_crop(image, [24, 24, 3])\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_brightness(image,max_delta=0.4)\n",
    "        image = tf.image.random_contrast(image,lower=0.5,upper=1.8)\n",
    "    \n",
    "    image = tf.image.convert_image_dtype(image,dtype=FLAGS.dtype)\n",
    "    # Ensure that the random shuffling has good mixing properties.\n",
    "    min_fraction_of_examples_in_queue = 0.4\n",
    "    NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "    min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *\n",
    "                             min_fraction_of_examples_in_queue)\n",
    "    \n",
    "    images, label_batch = tf.train.shuffle_batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=FLAGS.num_preprocess_threads,\n",
    "        capacity=min_queue_examples + 3 * batch_size,\n",
    "        min_after_dequeue=min_queue_examples,\n",
    "        seed=0)\n",
    "    \n",
    "    return [images,label_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "    \n",
    "    def weight_variable(shape,std=0.1,dtype=FLAGS.dtype):\n",
    "      initializer = tf.truncated_normal_initializer(stddev=std, dtype=dtype)\n",
    "      return tf.get_variable(\"Weights\",shape,initializer=initializer,dtype=dtype)\n",
    "\n",
    "    def bias_variable(shape,const=0.0,dtype=FLAGS.dtype):\n",
    "      initializer = tf.constant_initializer(const,dtype)\n",
    "      return tf.get_variable(\"biases\",shape,initializer=initializer,dtype=dtype)\n",
    "    \n",
    "    print(\"input : \", images)\n",
    "    \n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        W_conv1 = weight_variable([5, 5, 3, 64],std=5e-2)\n",
    "        b_conv1 = bias_variable([64],const=0.0)\n",
    "\n",
    "        conv = tf.nn.conv2d(images,W_conv1,strides=[1,1,1,1],padding=\"SAME\")\n",
    "        h_conv1 = tf.nn.relu(conv + b_conv1)\n",
    "        \n",
    "        print(\"conv 1 : \",h_conv1)\n",
    "        \n",
    "    with tf.variable_scope(\"maxpool1_norm\"):\n",
    "        h_pool1 = tf.nn.max_pool(h_conv1,ksize=[1,3,3,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "        h_norm1 = tf.nn.lrn(h_pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "        \n",
    "        print(\"pool_norm 1 : \",h_norm1)\n",
    "        \n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        W_conv2 = weight_variable([5, 5, 64, 64],std=5e-2)\n",
    "        b_conv2 = bias_variable([64],const=0.1)\n",
    "\n",
    "        conv = tf.nn.conv2d(h_norm1,W_conv2,strides=[1,1,1,1],padding=\"SAME\")\n",
    "        h_conv2 = tf.nn.relu(conv + b_conv2)\n",
    "        \n",
    "        print(\"conv 2 : \",h_conv2)\n",
    "        \n",
    "    with tf.variable_scope(\"norm_maxpool2\"):\n",
    "        h_norm2 = tf.nn.lrn(h_conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "        h_pool2 = tf.nn.max_pool(h_norm2,ksize=[1,3,3,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "        \n",
    "        print(\"norm_pool 2 : \",h_pool2)\n",
    "        \n",
    "    with tf.variable_scope(\"Flatten\"):\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [FLAGS.batch_size, -1])\n",
    "        \n",
    "        print(\"flatten : \",h_pool2_flat)\n",
    "        \n",
    "    with tf.variable_scope(\"fc1\"):\n",
    "        W_fc1 = weight_variable([h_pool2_flat.shape[1].value, 384],std=0.04)\n",
    "        b_fc1 = bias_variable([384],const=0.1)\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        \n",
    "        print(\"fc1 : \",h_fc1)\n",
    "        \n",
    "    with tf.variable_scope(\"fc2\"):\n",
    "        W_fc2 = weight_variable([384, 192],std=0.04)\n",
    "        b_fc2 = bias_variable([192],const=0.1)\n",
    "        h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "        \n",
    "        print(\"fc2 : \",h_fc2)\n",
    "        \n",
    "    with tf.variable_scope(\"logit\"):\n",
    "        W_fc3 = weight_variable([192, FLAGS.num_classes],std=1/192.0)\n",
    "        b_fc3 = bias_variable([FLAGS.num_classes],const=0.0)\n",
    "        logit = tf.nn.relu(tf.matmul(h_fc2, W_fc3) + b_fc3)\n",
    "        \n",
    "        print(\"logit : \",logit)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  Tensor(\"Input-queue/shuffle_batch:0\", shape=(128, 32, 32, 3), dtype=float32)\n",
      "conv 1 :  Tensor(\"conv1/Relu:0\", shape=(128, 32, 32, 64), dtype=float32)\n",
      "pool_norm 1 :  Tensor(\"maxpool1_norm/norm1:0\", shape=(128, 16, 16, 64), dtype=float32)\n",
      "conv 2 :  Tensor(\"conv2/Relu:0\", shape=(128, 16, 16, 64), dtype=float32)\n",
      "norm_pool 2 :  Tensor(\"norm_maxpool2/MaxPool:0\", shape=(128, 8, 8, 64), dtype=float32)\n",
      "flatten :  Tensor(\"Flatten/Reshape:0\", shape=(128, 4096), dtype=float32)\n",
      "fc1 :  Tensor(\"fc1/Relu:0\", shape=(128, 384), dtype=float32)\n",
      "fc2 :  Tensor(\"fc2/Relu:0\", shape=(128, 192), dtype=float32)\n",
      "logit :  Tensor(\"logit/Relu:0\", shape=(128, 10), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Logits:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.variable_scope(\"Input-queue\"):\n",
    "    images,labels = distorted_inputs(FLAGS.data_dir,FLAGS.batch_size,distort=False)\n",
    "    #tf.summary.image('images', images)\n",
    "    \n",
    "logit = inference(images)\n",
    "tf.summary.histogram(\"Logits\",logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = run_tf(logit)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(logit,labels):\n",
    "    \n",
    "    with tf.variable_scope(\"cross-entropy\"):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(labels,[FLAGS.batch_size]), logits=logit)\n",
    "        avg_cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "        return avg_cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularizing the weights of fc1 and fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizer(wd):\n",
    "    fc1_w = tf.trainable_variables(scope='fc1/Weights')[0]\n",
    "    wd_fc1 = tf.multiply(tf.nn.l2_loss(fc1_w), wd, name='fc1/weight_loss')\n",
    "    tf.summary.scalar(\"fc1_decay\",wd_fc1)\n",
    "\n",
    "    fc2_w = tf.trainable_variables(scope='fc2/Weights')[0]\n",
    "    wd_fc2 = tf.multiply(tf.nn.l2_loss(fc2_w), wd, name='fc2/weight_loss')\n",
    "    tf.summary.scalar(\"fc2_decay\",wd_fc2)\n",
    "    \n",
    "    return wd_fc1+wd_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'total_loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = tf.placeholder(FLAGS.dtype)\n",
    "decay_step = tf.placeholder(tf.int32)\n",
    "decay_rate = tf.placeholder(FLAGS.dtype)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "ce_loss = loss(logit,labels)\n",
    "tf.summary.scalar(\"mean_cross_entropy\",ce_loss)\n",
    "\n",
    "total_loss = ce_loss + regularizer(0.004)\n",
    "tf.summary.scalar(\"total_loss\",total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.train.exponential_decay(\n",
    "                                learning_rate=learning_rate,\n",
    "                                global_step=global_step,\n",
    "                                decay_steps=decay_step,\n",
    "                                decay_rate=decay_rate,\n",
    "                                staircase=True)\n",
    "tf.summary.scalar(\"learning_rate\",lr)\n",
    "opt = tf.train.GradientDescentOptimizer(lr)\n",
    "#opt = tf.train.AdamOptimizer(lr)\n",
    "grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "grad_to_compute = [[g,v] for g,v in grads if g is not None]\n",
    "# for grad,var in grad_to_compute:\n",
    "#     tf.summary.histogram(\"Params/\" + var.name,var)\n",
    "#     tf.summary.histogram(\"Gradients/\" + var.name,grad)\n",
    "    \n",
    "train_op = opt.apply_gradients(grads, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUN SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {learning_rate:0.1,\n",
    "            decay_rate : 0.1,\n",
    "            decay_step:80000}\n",
    "\n",
    "max_steps = 1700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss :  6.37e+00\n",
      "100 Loss :  6.02e+00\n",
      "200 Loss :  5.73e+00\n",
      "300 Loss :  5.40e+00\n",
      "400 Loss :  5.05e+00\n",
      "500 Loss :  4.65e+00\n",
      "600 Loss :  4.53e+00\n",
      "700 Loss :  4.04e+00\n",
      "800 Loss :  3.84e+00\n",
      "900 Loss :  3.61e+00\n",
      "1000 Loss :  3.41e+00\n",
      "1100 Loss :  3.12e+00\n",
      "1200 Loss :  2.98e+00\n",
      "1300 Loss :  2.77e+00\n",
      "1400 Loss :  2.76e+00\n",
      "1500 Loss :  2.26e+00\n",
      "1600 Loss :  2.49e+00\n"
     ]
    }
   ],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(config=config) as sess: \n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"log/Train_init_calc/regularized_lrn/t1_SGD_lr0.1\",sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        _,loss,out,summary = sess.run([train_op,total_loss,logit,merged],feed_dict=feed_dict)\n",
    "        writer.add_summary(summary,i)\n",
    "        if(i%100 == 0):\n",
    "            print(i, \"Loss : \", \"{:.2e}\".format(loss))\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
