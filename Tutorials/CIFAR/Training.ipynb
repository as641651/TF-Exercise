{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tf(x):\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    with tf.Session(config=config) as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        out = sess.run(x)\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class FLAGS(object):\n",
    "    pass\n",
    "\n",
    "FLAGS.batch_size = 128\n",
    "FLAGS.data_dir = \"/home/sankaran/exercise/ML/TF-Exercise/Tutorials/CIFAR/cifar-10-batches-bin\"\n",
    "FLAGS.num_preprocess_threads = 16\n",
    "FLAGS.num_classes = 10\n",
    "FLAGS.dtype = tf.float32\n",
    "\n",
    "def distorted_inputs(data_dir, batch_size, distort=True):\n",
    "    \n",
    "    filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)]\n",
    "    \n",
    "    # Create a queue that produces the filenames to read.\n",
    "    filename_queue = tf.train.string_input_producer(filenames,seed=0)\n",
    "    \n",
    "    #Create FixedLenthRecord Reader with fixed bytes to read\n",
    "    record_bytes = 32*32*3+1 #32*32*3 image with 1 byte for label\n",
    "    reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
    "    \n",
    "    key, value = reader.read(filename_queue)\n",
    "    \n",
    "    ##Decode\n",
    "    decoded = tf.decode_raw(value, tf.uint8)\n",
    "    label = tf.strided_slice(decoded,[0],[1])\n",
    "    image = tf.strided_slice(decoded,[1],[record_bytes])\n",
    "    \n",
    "    \n",
    "    label = tf.cast(label,tf.int32)\n",
    "    label = tf.reshape(label,[1])\n",
    "    image = tf.reshape(image,[3,32,32])\n",
    "    image = tf.transpose(image,[1,2,0])\n",
    "    #image = tf.cast(image,tf.float32) ## DESTROYES IMAGE VIS\n",
    "    \n",
    "    ##PRE PROCESS\n",
    "    if(distort):\n",
    "        image = tf.random_crop(image, [24, 24, 3])\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_brightness(image,max_delta=0.4)\n",
    "        image = tf.image.random_contrast(image,lower=0.5,upper=1.8)\n",
    "    \n",
    "    image = tf.image.convert_image_dtype(image,dtype=FLAGS.dtype)\n",
    "    # Ensure that the random shuffling has good mixing properties.\n",
    "    min_fraction_of_examples_in_queue = 0.4\n",
    "    NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "    min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *\n",
    "                             min_fraction_of_examples_in_queue)\n",
    "    \n",
    "    images, label_batch = tf.train.shuffle_batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=FLAGS.num_preprocess_threads,\n",
    "        capacity=min_queue_examples + 3 * batch_size,\n",
    "        min_after_dequeue=min_queue_examples,\n",
    "        seed=0)\n",
    "    \n",
    "    return [images,label_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "    \n",
    "    def weight_variable(shape,std=0.1,dtype=FLAGS.dtype):\n",
    "      initializer = tf.truncated_normal_initializer(stddev=std, dtype=dtype)\n",
    "      return tf.get_variable(\"Weights\",shape,initializer=initializer,dtype=dtype)\n",
    "\n",
    "    def bias_variable(shape,const=0.0,dtype=FLAGS.dtype):\n",
    "      initializer = tf.constant_initializer(const,dtype)\n",
    "      return tf.get_variable(\"biases\",shape,initializer=initializer,dtype=dtype)\n",
    "    \n",
    "    print(\"input : \", images)\n",
    "    \n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        W_conv1 = weight_variable([5, 5, 3, 64])\n",
    "        b_conv1 = bias_variable([64])\n",
    "\n",
    "        conv = tf.nn.conv2d(images,W_conv1,strides=[1,1,1,1],padding=\"SAME\")\n",
    "        h_conv1 = tf.nn.relu(conv + b_conv1)\n",
    "        \n",
    "        print(\"conv 1 : \",h_conv1)\n",
    "        \n",
    "    with tf.variable_scope(\"maxpool1\"):\n",
    "        h_pool1 = tf.nn.max_pool(h_conv1,ksize=[1,3,3,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "        \n",
    "        print(\"pool 1 : \",h_pool1)\n",
    "        \n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        W_conv2 = weight_variable([5, 5, 64, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "\n",
    "        conv = tf.nn.conv2d(h_pool1,W_conv2,strides=[1,1,1,1],padding=\"SAME\")\n",
    "        h_conv2 = tf.nn.relu(conv + b_conv2)\n",
    "        \n",
    "        print(\"conv 2 : \",h_conv2)\n",
    "        \n",
    "    with tf.variable_scope(\"maxpool2\"):\n",
    "        h_pool2 = tf.nn.max_pool(h_conv2,ksize=[1,3,3,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "        \n",
    "        print(\"pool 2 : \",h_pool2)\n",
    "        \n",
    "    with tf.variable_scope(\"Flatten\"):\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [FLAGS.batch_size, -1])\n",
    "        \n",
    "        print(\"flatten : \",h_pool2_flat)\n",
    "        \n",
    "    with tf.variable_scope(\"fc1\"):\n",
    "        W_fc1 = weight_variable([h_pool2_flat.shape[1].value, 384])\n",
    "        b_fc1 = bias_variable([384])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        \n",
    "        print(\"fc1 : \",h_fc1)\n",
    "        \n",
    "    with tf.variable_scope(\"fc2\"):\n",
    "        W_fc2 = weight_variable([384, 192])\n",
    "        b_fc2 = bias_variable([192])\n",
    "        h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "        \n",
    "        print(\"fc2 : \",h_fc2)\n",
    "        \n",
    "    with tf.variable_scope(\"logit\"):\n",
    "        W_fc3 = weight_variable([192, FLAGS.num_classes])\n",
    "        b_fc3 = bias_variable([FLAGS.num_classes])\n",
    "        logit = tf.nn.relu(tf.matmul(h_fc2, W_fc3) + b_fc3)\n",
    "        \n",
    "        print(\"logit : \",logit)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  Tensor(\"Input-queue/shuffle_batch:0\", shape=(128, 32, 32, 3), dtype=float32)\n",
      "conv 1 :  Tensor(\"conv1/Relu:0\", shape=(128, 32, 32, 64), dtype=float32)\n",
      "pool 1 :  Tensor(\"maxpool1/MaxPool:0\", shape=(128, 16, 16, 64), dtype=float32)\n",
      "conv 2 :  Tensor(\"conv2/Relu:0\", shape=(128, 16, 16, 64), dtype=float32)\n",
      "pool 2 :  Tensor(\"maxpool2/MaxPool:0\", shape=(128, 8, 8, 64), dtype=float32)\n",
      "flatten :  Tensor(\"Flatten/Reshape:0\", shape=(128, 4096), dtype=float32)\n",
      "fc1 :  Tensor(\"fc1/Relu:0\", shape=(128, 384), dtype=float32)\n",
      "fc2 :  Tensor(\"fc2/Relu:0\", shape=(128, 192), dtype=float32)\n",
      "logit :  Tensor(\"logit/Relu:0\", shape=(128, 10), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Logits:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.variable_scope(\"Input-queue\"):\n",
    "    images,labels = distorted_inputs(FLAGS.data_dir,FLAGS.batch_size,distort=False)\n",
    "    tf.summary.image('images', images)\n",
    "    \n",
    "logit = inference(images)\n",
    "tf.summary.histogram(\"Logits\",logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = run_tf(logit)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(logit,lables):\n",
    "    \n",
    "    with tf.variable_scope(\"cross-entropy\"):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(labels,[FLAGS.batch_size]), logits=logit)\n",
    "        avg_cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "        return avg_cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DECAYING LEARNING RATE**\n",
    "\n",
    "**tf.train.exponential_decay**\n",
    "\n",
    "Reduces learning rate exponentially every **decay_steps**\n",
    "\n",
    "decayed_learning_rate = learning_rate X decay_rate ^ (global_step / decay_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007943282"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = tf.train.exponential_decay(learning_rate=0.01,global_step=10,decay_steps=100,decay_rate=0.1)\n",
    "tf.Session().run(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = tf.train.exponential_decay(0.01,100,100,0.1)\n",
    "tf.Session().run(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000100000005"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = tf.train.exponential_decay(0.01,200,100,0.1)\n",
    "tf.Session().run(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = []\n",
    "for i in range(1000):\n",
    "    e.append(tf.train.exponential_decay(0.1,i,400,0.01,staircase=True))\n",
    "e_lr = tf.Session().run(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f839ed1ee10>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f83fd4ab390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(e_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mean_cross_entropy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = tf.placeholder(FLAGS.dtype)\n",
    "decay_step = tf.placeholder(tf.int32)\n",
    "decay_rate = tf.placeholder(FLAGS.dtype)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "total_loss = loss(logit,labels)\n",
    "tf.summary.scalar(\"mean_cross_entropy\",total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Params/conv1/Weights:0 is illegal; using Params/conv1/Weights_0 instead.\n",
      "INFO:tensorflow:Summary name Gradients/conv1/Weights:0 is illegal; using Gradients/conv1/Weights_0 instead.\n",
      "INFO:tensorflow:Summary name Params/conv1/biases:0 is illegal; using Params/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Gradients/conv1/biases:0 is illegal; using Gradients/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Params/conv2/Weights:0 is illegal; using Params/conv2/Weights_0 instead.\n",
      "INFO:tensorflow:Summary name Gradients/conv2/Weights:0 is illegal; using Gradients/conv2/Weights_0 instead.\n",
      "INFO:tensorflow:Summary name Params/conv2/biases:0 is illegal; using Params/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Gradients/conv2/biases:0 is illegal; using Gradients/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Params/fc1/Weights:0 is illegal; using Params/fc1/Weights_0 instead.\n",
      "INFO:tensorflow:Summary name Gradients/fc1/Weights:0 is illegal; using Gradients/fc1/Weights_0 instead.\n",
      "INFO:tensorflow:Summary name Params/fc1/biases:0 is illegal; using Params/fc1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Gradients/fc1/biases:0 is illegal; using Gradients/fc1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Params/fc2/Weights:0 is illegal; using Params/fc2/Weights_0 instead.\n",
      "INFO:tensorflow:Summary name Gradients/fc2/Weights:0 is illegal; using Gradients/fc2/Weights_0 instead.\n",
      "INFO:tensorflow:Summary name Params/fc2/biases:0 is illegal; using Params/fc2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Gradients/fc2/biases:0 is illegal; using Gradients/fc2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Params/logit/Weights:0 is illegal; using Params/logit/Weights_0 instead.\n",
      "INFO:tensorflow:Summary name Gradients/logit/Weights:0 is illegal; using Gradients/logit/Weights_0 instead.\n",
      "INFO:tensorflow:Summary name Params/logit/biases:0 is illegal; using Params/logit/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Gradients/logit/biases:0 is illegal; using Gradients/logit/biases_0 instead.\n"
     ]
    }
   ],
   "source": [
    "lr = tf.train.exponential_decay(\n",
    "                                learning_rate=learning_rate,\n",
    "                                global_step=global_step,\n",
    "                                decay_steps=decay_step,\n",
    "                                decay_rate=decay_rate,\n",
    "                                staircase=True)\n",
    "tf.summary.scalar(\"learning_rate\",lr)\n",
    "# opt = tf.train.GradientDescentOptimizer(lr)\n",
    "opt = tf.train.AdamOptimizer(lr)\n",
    "grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "grad_to_compute = [[g,v] for g,v in grads if g is not None]\n",
    "for grad,var in grad_to_compute:\n",
    "    tf.summary.histogram(\"Params/\" + var.name,var)\n",
    "    tf.summary.histogram(\"Gradients/\" + var.name,grad)\n",
    "    \n",
    "train_op = opt.apply_gradients(grads, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUN SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {learning_rate:0.001,\n",
    "            decay_rate : 0.1,\n",
    "            decay_step:80000}\n",
    "\n",
    "max_steps = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss :  8.30e+00 [ 0.          0.          0.          0.2792165   0.04017529  7.972145\n",
      "  7.080236   10.317077    3.2572646  10.926981  ]\n",
      "1 Loss :  6.34e+00 [ 0.         0.         0.        10.147064   7.9769306  0.\n",
      "  3.0278103  8.589553  11.311187   0.       ]\n",
      "2 Loss :  9.17e+00 [ 0.          0.          0.          1.675221   13.069405    0.\n",
      "  3.8971767   2.3095531   0.48577443  0.        ]\n",
      "3 Loss :  7.11e+00 [0.        0.        0.        1.4428666 0.        0.        9.698742\n",
      " 0.        0.        0.       ]\n",
      "4 Loss :  3.16e+00 [1.5733446  0.         0.         3.7408354  0.         0.\n",
      " 0.42208424 0.         0.         0.        ]\n",
      "5 Loss :  2.53e+00 [2.451442 0.       0.       0.       0.       0.       0.       0.\n",
      " 0.       0.      ]\n",
      "6 Loss :  2.32e+00 [0.42476594 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "7 Loss :  2.30e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "8 Loss :  2.27e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "9 Loss :  2.30e+00 [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.5497329 0.       ]\n",
      "10 Loss :  2.26e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "11 Loss :  2.28e+00 [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.6745754 0.       ]\n",
      "12 Loss :  2.28e+00 [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.30381826 0.        ]\n",
      "13 Loss :  2.25e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "14 Loss :  2.26e+00 [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.30004156 0.        ]\n",
      "15 Loss :  2.27e+00 [0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 2.355275 0.      ]\n",
      "16 Loss :  2.27e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "17 Loss :  2.25e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "18 Loss :  2.28e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "19 Loss :  2.25e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20 Loss :  2.24e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "21 Loss :  2.36e+00 [0.      0.      0.      0.      0.      0.      0.      0.      1.48783\n",
      " 0.     ]\n",
      "22 Loss :  2.24e+00 [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.4754478 0.       ]\n",
      "23 Loss :  2.30e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "24 Loss :  2.28e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "25 Loss :  2.30e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "26 Loss :  2.30e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "27 Loss :  2.30e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "28 Loss :  2.30e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "29 Loss :  2.29e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "30 Loss :  2.29e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "31 Loss :  2.28e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "32 Loss :  2.27e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "33 Loss :  2.28e+00 [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.1821837 0.       ]\n",
      "34 Loss :  2.25e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "35 Loss :  2.25e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "36 Loss :  2.28e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "37 Loss :  2.23e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "38 Loss :  2.30e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "39 Loss :  2.26e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "40 Loss :  2.31e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "41 Loss :  2.27e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "42 Loss :  2.27e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "43 Loss :  2.27e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "44 Loss :  2.29e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "45 Loss :  2.22e+00 [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.4976624 0.       ]\n",
      "46 Loss :  2.25e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "47 Loss :  2.24e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "48 Loss :  2.27e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "49 Loss :  2.28e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "50 Loss :  2.26e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "51 Loss :  2.27e+00 [0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 2.063614 0.      ]\n",
      "52 Loss :  2.29e+00 [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0401073 0.       ]\n",
      "53 Loss :  2.24e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "54 Loss :  2.23e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "55 Loss :  2.24e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "56 Loss :  2.19e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "57 Loss :  2.28e+00 [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.9479685 0.       ]\n",
      "58 Loss :  2.24e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "59 Loss :  2.27e+00 [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01707369 0.        ]\n",
      "60 Loss :  2.27e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "61 Loss :  2.24e+00 [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        3.1137042 0.       ]\n",
      "62 Loss :  2.28e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "63 Loss :  2.20e+00 [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.8742364 0.       ]\n",
      "64 Loss :  2.26e+00 [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.6751693 0.       ]\n",
      "65 Loss :  2.24e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "66 Loss :  2.18e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "67 Loss :  2.27e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "68 Loss :  2.29e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "69 Loss :  2.16e+00 [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.6990302 0.       ]\n",
      "70 Loss :  2.26e+00 [0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 2.427786 0.      ]\n",
      "71 Loss :  2.23e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "72 Loss :  2.23e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "73 Loss :  2.21e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "74 Loss :  2.20e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "75 Loss :  2.23e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "76 Loss :  2.29e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "77 Loss :  2.27e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "78 Loss :  2.25e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "79 Loss :  2.20e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "80 Loss :  2.24e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "81 Loss :  2.24e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "82 Loss :  2.19e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "83 Loss :  2.25e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "84 Loss :  2.21e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "85 Loss :  2.16e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "86 Loss :  2.40e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "87 Loss :  2.16e+00 [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.1277605 0.       ]\n",
      "88 Loss :  2.24e+00 [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11882447 0.        ]\n",
      "89 Loss :  2.23e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "90 Loss :  2.18e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "91 Loss :  2.24e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "92 Loss :  2.21e+00 [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.6737955 0.       ]\n",
      "93 Loss :  2.24e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "94 Loss :  2.27e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "95 Loss :  2.22e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "96 Loss :  2.26e+00 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: Input-queue/shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Input-queue/shuffle_batch/random_shuffle_queue, Input-queue/convert_image/_17, Input-queue/Cast/_19)]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b2160594ecde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         if(i%1000 == 0):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(config=config) as sess: \n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"log/Train_1\",sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        _,loss,out,summary = sess.run([train_op,total_loss,logit,merged],feed_dict=feed_dict)\n",
    "        writer.add_summary(summary,i)\n",
    "#         if(i%1000 == 0):\n",
    "        print(i, \"Loss : \", \"{:.2e}\".format(loss),out[0])\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tf(tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 3, 0, 1, 1, 7, 1, 9, 1, 4, 3, 1, 6, 5, 9, 1, 4, 0, 2, 5, 9,\n",
       "       5, 6, 0, 5, 1, 1, 6, 2, 0, 2, 1, 5, 0, 4, 0, 7, 3, 0, 5, 6, 9, 2,\n",
       "       5, 4, 9, 6, 6, 4, 3, 7, 0, 1, 0, 4, 3, 2, 7, 1, 5, 1, 2, 3, 8, 5,\n",
       "       3, 5, 7, 2, 2, 4, 5, 5, 9, 4, 2, 2, 4, 8, 3, 8, 6, 9, 6, 9, 5, 2,\n",
       "       1, 8, 7, 0, 1, 9, 1, 5, 4, 8, 5, 5, 3, 7, 1, 5, 2, 9, 3, 1, 5, 5,\n",
       "       5, 0, 0, 3, 9, 5, 8, 6, 6, 6, 0, 7, 4, 3, 8, 9, 9, 5], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tf(tf.reshape(labels,[FLAGS.batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_to_compute = [[g,v] for g,v in grads if g is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1/Weights:0\n",
      "conv1/biases:0\n",
      "conv2/Weights:0\n",
      "conv2/biases:0\n",
      "fc1/Weights:0\n",
      "fc1/biases:0\n",
      "fc2/Weights:0\n",
      "fc2/biases:0\n",
      "logit/Weights:0\n",
      "logit/biases:0\n"
     ]
    }
   ],
   "source": [
    "for g,v in grad_to_compute:\n",
    "    print(v.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
